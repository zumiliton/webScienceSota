\documentclass[10pt]{beamer}

%--------------------------------
% PAQUETES BÁSICOS
%--------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{ragged2e}
\usepackage{lipsum}

%--------------------------------
% TEMA VISUAL
%--------------------------------
\usetheme{CambridgeUS} % Puedes probar: "Boadilla", "Berkeley", "Warsaw", "Copenhagen", "Singapore", etc.
\usecolortheme{dolphin} % Cambia el esquema de color

% Quita la barra de navegación inferior
\setbeamertemplate{navigation symbols}{}

% Título centrado
\setbeamertemplate{frametitle}[default][center]

%--------------------------------
% DATOS DE LA PRESENTACIÓN
%--------------------------------
\title[sotaWebScience]{LLMs y Sentimientos}
\subtitle{Influencia de los LLMs en el Procesamiento y Análisis de Sentimientos}
\author{Nadine San Millan, Manuel Garcés, Carlos Navarro, Marcos León}
\institute{Universidad Politécnica de Madrid}
\date{\today}

%--------------------------------
% INICIO DEL DOCUMENTO
%--------------------------------
\begin{document}
	
	% PORTADA
	\begin{frame}
		\titlepage
	\end{frame}

    \AtBeginSection[]{
      \begin{frame}
        \frametitle{Contenido}
        \tableofcontents[currentsection]
      \end{frame}
    }

	
	% ÍNDICE
	\begin{frame}
		\frametitle{Contenido}
		\tableofcontents
	\end{frame}
    
    \section{Contexto general}
    \begin{frame}
      \frametitle{Contexto general}
      \framesubtitle{Por qué nos importa el Análisis de Sentimientos hoy}
      \begin{itemize}
        \item La cantidad de texto generado en \textbf{redes sociales}, reseñas, foros y medios digitales ha crecido de forma explosiva.
        \item El \textbf{Sentiment Analysis (SA)} busca extraer opiniones, emociones y valoraciones a partir de ese texto.
        \item Tiene aplicaciones directas en:
        \begin{itemize}
          \item \textbf{Marketing} y análisis de marca.
          \item \textbf{Atención al cliente} y reputación online.
          \item \textbf{Análisis político} y de opinión pública.
        \end{itemize}
        \item Tradicionalmente se han usado \textbf{modelos específicos por tarea y dominio} (por ejemplo, reseñas de hoteles, restaurantes, productos, etc.).
      \end{itemize}
    \end{frame}
    
    \section{Motivación}
    \begin{frame}
      \frametitle{LLMs y motivación del estudio}
      \framesubtitle{¿Reemplazan los LLMs a los modelos especializados?}
      \begin{itemize}
        \item La aparición de los \textbf{Large Language Models (LLMs)} (GPT-3.5, GPT-4, PaLM, etc.) ha cambiado el panorama del NLP.
        \item Estos modelos pueden resolver tareas de SA en modo:
        \begin{itemize}
          \item \textbf{Zero-shot} y \textbf{few-shot}, solo ajustando el \textbf{prompt}.
          \item Sin necesidad de entrenar un modelo nuevo para cada dominio.
        \end{itemize}
        \item Sin embargo, surgen preguntas clave:
        \begin{itemize}
          \item ¿Rinden tan bien como los \textbf{modelos especializados} entrenados para SA/ABSA?
          \item ¿Cómo afectan el \textbf{dominio}, la disponibilidad de \textbf{datos etiquetados} y el diseño del \textbf{prompt}?
          \item ¿En qué escenarios compensa su \textbf{coste computacional}?
        \end{itemize}
        \item Esta presentación intenta responder a estas preguntas a partir de \textbf{tres artículos recientes} sobre LLMs y análisis de sentimientos.
      \end{itemize}
    \end{frame}
    
	
	% SECCIÓN 1
	\section{Sentiment Analysis in the Era of Large Language Models: A Reality Check}
	\begin{frame}
		\frametitle{Introducción}
		\framesubtitle{Análisis de Sentimientos en la Era de los Grandes Modelos de Lenguaje (LLMs): Una Verificación de la Realidad}
		\begin{itemize}
			\item \textbf{Introducción al Análisis de Sentimientos (SA):} El SA es un área de investigación establecida en el procesamiento del lenguaje natural (NLP). Busca estudiar las opiniones, sentimientos y emociones de las personas a través de métodos computacionales.
			\item \textbf{El Boom de los LLMs:} Los LLMs (como GPT-3.5/ChatGPT) han demostrado un rendimiento impresionante en varias tareas de NLP.
			\item \textbf{Pregunta de Investigación:} ¿Hasta qué punto se pueden aprovechar los LLMs actuales para las diferentes tareas de SA? Este estudio busca una “verificación de la realidad” sobre el estado actual del SA con LLMs.
		\end{itemize}
	\end{frame}
	
	\begin{frame}
		\frametitle{Evaluación}
		\framesubtitle{Alcance de la Evaluación: 13 Tareas, 26 Datasets y Comparación con SLMs}
		\begin{itemize}
			\item \textbf{Modelos Evaluados (LLMs):} Se consideraron modelos de código abierto (Flan-T5, Flan-UL2) y modelos de la serie GPT-3.5 (ChatGPT, InstructGPT/text-davinci-003).
			\item \textbf{Modelos de Comparación (SLMs):} Se utilizaron modelos de lenguaje más pequeños (Small Language Models - SLMs) como T5 (versión large, 770M), entrenados con conjuntos de datos específicos del dominio (in-domain labeled data).
			\item \textbf{Evaluación Integral:} La investigación cubrió 13 tareas distintas de SA en 26 conjuntos de datos.
			\item \textbf{Categorías de Tareas de SA Investigadas:}
			\begin{enumerate}
				\item \textbf{Clasificación de Sentimientos (SC):} Clasificar la orientación sentimental de un texto (a nivel de documento, oración o aspecto).
				\item \textbf{Análisis de Sentimientos Basado en Aspectos (ABSA):} Análisis de sentimientos y opiniones a un nivel de aspecto más detallado.
				\item \textbf{Análisis Multifacético de Textos Subjetivos (MAST):} Tareas centradas en fenómenos subjetivos específicos (p. ej., detección de hate speech, ironía, reconocimiento de emociones).
			\end{enumerate}
		\end{itemize}
	\end{frame}
	
	
	\begin{frame}
		\frametitle{Ilustración diferentes PROMPTs}
		\begin{figure}[htbp]
			\centering
			\includegraphics[width=0.95\textwidth]{sensibility.png}
			\caption{Prompt examples for SC, ABSA, and MAST respectively.}
		\end{figure}
		
		% Si quieres agregar otra imagen, puedes descomentar lo siguiente:
		% \begin{figure}[htbp]
			%     \centering
			%     \includegraphics[width=0.6\textwidth]{ruta/de/otra/imagen.png}
			%     \caption{Otra descripción.}
			% \end{figure}
	\end{frame}
	
	\begin{frame}
		\frametitle{Hallazgo 1: Rendimiento Cero-Shot}
		\framesubtitle{LLMs vs. SLMs: Brechas de Rendimiento en Configuración Cero-Shot (Zero-Shot)}
		\begin{itemize}
			\item \textbf{Tareas Simples (SC):} Los LLMs (como ChatGPT) muestran una fuerte capacidad de SA en configuraciones cero-shot. En algunas tareas SC, los LLMs pueden rendir a la par de los SLMs ajustados con el conjunto de entrenamiento completo.
			\begin{itemize}
				\item \textit{Ejemplo:} En promedio, el rendimiento de ChatGPT alcanzó el 97\% del rendimiento del modelo T5 ajustado en tareas SC.
			\end{itemize}
			\item \textbf{Tareas Complejas (ABSA y MAST):} Los LLMs aún están por detrás de los SLMs entrenados con datos específicos del dominio en tareas que requieren un conocimiento más profundo o información estructurada.
			\begin{itemize}
				\item \textit{Desafío ABSA:} Tareas como la Extracción Unificada de Sentimientos Basada en Aspectos (UABSA) o la Extracción de Tripletes (ASTE) requieren información de sentimiento estructurada y de grano fino. Modelos como \texttt{text-davinci-003} solo alcanzaron alrededor del 54\% del rendimiento de un modelo T5 ajustado en tareas ABSA.
				\item \textit{Desafío MAST:} Los LLMs también van a la zaga en tareas MAST complejas, como el análisis de sentimiento implícito.
			\end{itemize}
		\end{itemize}
	\end{frame}
	
	
	\begin{frame}
		\frametitle{Hallazgo 2: Aprendizaje Few-Shot}
		\framesubtitle{Ventaja Estratégica: LLMs Superan a SLMs en Escenarios Few-Shot}
		\begin{itemize}
			\item \textbf{Superioridad Consistente:} Los LLMs (con \textit{in-context learning}) superan consistentemente a los SLMs entrenados con la misma cantidad limitada de datos en todas las categorías de tareas (\textbf{SC}, \textbf{ABSA}, \textbf{MAST}) en configuraciones \textit{few-shot} (p. ej., 1-shot, 5-shot, 10-shot).
			\item \textbf{Implicación Práctica:} Esto sugiere que la aplicación de LLMs es ventajosa cuando los recursos de anotación son escasos.
			\item \textbf{La Brecha de Datos:} \texttt{ChatGPT} establece una base sólida que requiere que el modelo \texttt{T5} utilice casi cinco a diez veces más datos (p. ej., 50-shot o 100-shot) para lograr un rendimiento comparable en promedio.
			\item \textbf{Impacto de la Complejidad del Prompt:} El beneficio incremental de añadir más ejemplos (\textit{shots}) es menos obvio para tareas \textbf{SC} más simples, pero aumenta considerablemente el rendimiento de los LLMs en tareas \textbf{ABSA} que exigen una comprensión más profunda y un formato de salida preciso.
		\end{itemize}
	\end{frame}
	
	
		\begin{frame}
		\frametitle{Resultados FEW-SHOT}
		\begin{figure}[htbp]
			\centering
			\includegraphics[width=1\textwidth]{images/fewShot.jpg}
			\caption{Averaged few-shot results on all datasets for each task type with an increasing number of different shots.}
		\end{figure}
		
		% Si quieres agregar otra imagen, puedes descomentar lo siguiente:
		% \begin{figure}[htbp]
			%     \centering
			%     \includegraphics[width=0.6\textwidth]{ruta/de/otra/imagen.png}
			%     \caption{Otra descripción.}
			% \end{figure}
	\end{frame}
	
	\begin{frame}
		\frametitle{Desafio clave: Sensibilidad a Prompts}
		\framesubtitle{Fragilidad de los LLMs: La Sensibilidad al Diseño del Prompt}
		\begin{itemize}
			\item \textbf{Variabilidad Observada:} El diseño de \textit{prompts} adecuados es fundamental. Los LLMs pueden producir respuestas muy diferentes incluso con \textit{prompts} semánticamente similares.
			\item \textbf{Impacto de la Complejidad de la Tarea:}
			\begin{itemize}
				\item En tareas \textbf{SC} (clasificación simple), la elección del \textit{prompt} parece tener un efecto menor.
				\item En tareas que requieren una salida estructurada y de grano fino (\textbf{ABSA}), el rendimiento puede variar significativamente (mayor sensibilidad) dependiendo del diseño del \textit{prompt}.
				\item \textit{Detalle:} Se encontró que los modelos pueden ser sensibles a ciertas palabras, como \textit{analyze}, donde el modelo podría generar explicaciones largas a pesar de recibir instrucciones explícitas para no hacerlo.
			\end{itemize}
		\end{itemize}
	\end{frame}
	
	
	
	\begin{frame}
		\frametitle{Sensibilidad de PROMPTs}
		\begin{figure}[htbp]
			\centering
			\includegraphics[width=1\textwidth]{images/typesSensi.jpg}
			\caption{Sensitivity of different prompt designs on three types of SA tasks.}
		\end{figure}
		
		% Si quieres agregar otra imagen, puedes descomentar lo siguiente:
		% \begin{figure}[htbp]
			%     \centering
			%     \includegraphics[width=0.6\textwidth]{ruta/de/otra/imagen.png}
			%     \caption{Otra descripción.}
			% \end{figure}
	\end{frame}
	
	
	\begin{frame}
		\frametitle{Prompt Estructurado}
		\framesubtitle{Ilustración de la Complejidad en Tareas ABSA}
		\begin{block}{Ejemplo de Prompt para UABSA}
			Para la tarea UABSA (Unified Aspect-Based Sentiment Analysis), un LLM podría recibir un prompt como:
			
			\textit{``Extrae los aspectos y clasifica el sentimiento de cada uno en el siguiente texto, proporcionando la salida en formato JSON estructurado con las claves 'aspect', 'sentiment' y 'evidence'."}
		\end{block}
	\end{frame}

	\begin{frame}
		\frametitle{Limitaciones y Benchmark}
		\framesubtitle{Limitaciones de la Evaluación Actual y el Nuevo Benchmark SENTIEVAL}
		\begin{itemize}
			\item \textbf{Deficiencias de las Prácticas de Evaluación Actuales:} Las evaluaciones a menudo se centran estrechamente en tareas o conjuntos de datos específicos, y utilizan \textit{prompts} inconsistentes entre estudios. Esto no logra capturar la amplitud total de las capacidades de \textbf{SA} de un \textbf{LLM}.
			\item \textbf{Propuesta SENTIEVAL:} Los autores proponen un nuevo \textit{benchmark} llamado \texttt{SENTIEVAL} para una evaluación más integral y realista.
			\begin{itemize}
				\item \textit{Objetivo 1:} Romper la barrera entre las tareas individuales de \textbf{SA} para una evaluación más integral.
				\item \textit{Objetivo 2:} Evaluar el modelo utilizando instrucciones de lenguaje natural presentadas en varios estilos. Esto imita el uso real donde los humanos interactúan con el modelo de diversas maneras.
				\item \textit{Resultado:} \texttt{SENTIEVAL} requiere que el modelo comprenda diversos estilos de instrucciones y cumpla con formatos requeridos, un desafío donde \texttt{ChatGPT} demostró mayor robustez en comparación con \texttt{Flan-UL2}.
			\end{itemize}
		\end{itemize}
	\end{frame}
	
	\begin{frame}
		\frametitle{Conclusiones Clave}
		\framesubtitle{Conclusiones Clave para la Adopción Práctica de LLMs en SA}
		\begin{itemize}
			\item \textbf{LLMs son Soluciones Viables para Tareas Simples:} Para clasificación binaria o trinaria simple, los LLMs pueden ser soluciones efectivas, incluso en modo cero-shot.
			\item \textbf{Ventaja en Escasez de Datos:} Cuando los recursos de anotación son limitados, los LLMs son una buena opción debido a su rendimiento superior en el aprendizaje \textit{few-shot}.
			\item \textbf{Precaución con Tareas Estructuradas:} Para tareas que requieren una salida estructurada de sentimientos (\textbf{ABSA}), los LLMs pueden no ser la mejor opción y su rendimiento puede variar significativamente con diferentes \textit{prompts}.
			\item \textbf{Tamaño del Modelo vs. Instruction-Tuning:} Los modelos más grandes no siempre garantizan un rendimiento superior (p. ej., \texttt{Flan-UL2} fue comparable a la serie \texttt{GPT-3.5} en algunos casos). El uso de \textit{Instruction-Tuning} en modelos de tamaño razonable puede ser suficiente para aplicaciones prácticas de \textbf{SA}.
		\end{itemize}
		
	\end{frame}

    
	% SECCIÓN 2
	\section{Large Language Models for Aspect-Based Sentiment Analysis}
	\begin{frame}
    \frametitle{Introducción}
    \framesubtitle{Modelos GPT aplicados al Análisis de Sentimientos Basado en Aspectos (ABSA)}
    \begin{itemize}
        \item ABSA busca identificar aspectos específicos en un texto y clasificar su polaridad.
        \item El artículo evalúa cómo GPT-3.5 y GPT-4 rinden en ABSA comparados con modelos especializados.
        \item Se analizan tres configuraciones: \textbf{zero-shot}, \textbf{few-shot} y \textbf{fine-tuning}.
        \item Pregunta clave: ¿qué funciona mejor, \textbf{prompt engineering} o \textbf{fine-tuning}?
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Metodología}
    \framesubtitle{Dataset, modelos y tareas evaluadas}
    \begin{itemize}
        \item Dataset: \textbf{SemEval-2014 Task 4} (restaurantes y laptops).
        \item Tareas:
        \begin{itemize}
            \item Extracción de términos de aspecto.
            \item Clasificación de polaridad.
            \item Tarea conjunta (la usada en el estudio).
        \end{itemize}
        \item Modelos evaluados:
        \begin{itemize}
            \item GPT-3.5 (sin ajustar)
            \item GPT-3.5 \textbf{fine-tuned}
            \item GPT-4
            \item Modelo especializado \textbf{InstructABSA}
        \end{itemize}
        \item Se han probado 9 prompts distintos y 0–10 ejemplos en contexto.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Resultados: Diseño de Prompts}
    \framesubtitle{Impacto del prompt y del número de ejemplos}
    \begin{itemize}
        \item El mejor prompt sin entrenamiento fue \textbf{Guidelines summary}.
        \item Añadir ejemplos mejora marginalmente el rendimiento.
        \item Usar 6 ejemplos funciona mejor que usar 10 (los casos difíciles añaden ruido).
        \item Los LLMs muestran alta sensibilidad al diseño del prompt en ABSA.
    \end{itemize}
\end{frame}

% -----------------------------
% IMAGEN 1: DINÁMICA ENTRENAMIENTO
% -----------------------------
\begin{frame}
    \frametitle{Fine-Tuning: Dinámica del Entrenamiento}
    \framesubtitle{Evolución del modelo ajustado}

    \centering
    \includegraphics[width=0.90\textwidth]{img_3_1.png}

    \vspace{0.2cm}
    {\footnotesize Curvas de entrenamiento y validación del GPT-3.5 fine-tuned.\par}
\end{frame}

\begin{frame}
    \frametitle{Fine-Tuning: Resultados Numéricos}
    \framesubtitle{GPT-3.5 ajustado supera el estado del arte}
    \begin{itemize}
        \item GPT-3.5 fine-tuned logra un \textbf{F1 = 83.8}, superando:
        \begin{itemize}
            \item a GPT-4 zero/few-shot
            \item y al modelo especializado InstructABSA (+5.7 puntos)
        \end{itemize}
        \item El modelo ajustado:
        \begin{itemize}
            \item No necesita JSON schema.
            \item No necesita prompts largos.
            \item Es mucho más robusto que la versión sin entrenamiento.
        \end{itemize}
        \item Conclusión clave: \textbf{fine-tuning $>$ prompt engineering}.
    \end{itemize}
\end{frame}

% -----------------------------
% IMAGEN 2: COMPARACIÓN DE MODELOS
% -----------------------------
\begin{frame}
    \frametitle{Comparación de Modelos}
    \framesubtitle{Rendimiento con diferentes números de ejemplos}

    \centering
    \includegraphics[width=0.93\textwidth]{img_5_2.png}

    \vspace{0.2cm}
    {\footnotesize Comparación del rendimiento entre GPT-3.5, GPT-3.5 fine-tuned, GPT-4 e InstructABSA.\par}
\end{frame}

\begin{frame}
    \frametitle{Análisis de Errores}
    \framesubtitle{¿En qué fallan los LLMs?}
    \begin{itemize}
        \item Tipos de errores encontrados:
        \begin{enumerate}
            \item Aspectos no presentes en el gold set.
            \item Polaridad incorrecta.
            \item Límites erróneos del término.
            \item Términos inventados.
        \end{enumerate}
        \item Tras el fine-tuning:
        \begin{itemize}
            \item Los errores de tipo “aspecto no válido” se reducen un \textbf{88\%}.
            \item Los términos inventados prácticamente desaparecen.
        \end{itemize}
        \item Los errores restantes reflejan sobre todo las \textbf{restricciones del dataset SemEval}. 
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Análisis Económico}
    \framesubtitle{Coste vs. rendimiento}
    \begin{itemize}
        \item GPT-4 es el modelo más caro y menos eficiente por dólar.
        \item GPT-3.5 fine-tuned es el más eficiente dentro de los GPT.
        \item InstructABSA es extremadamente barato y competitivo.
        \item Conclusión práctica:
        \begin{itemize}
            \item Si hay datos etiquetados → \textbf{conviene fine-tuning}.
            \item Si no hay datos → \textbf{usar GPT-4 zero/few-shot}.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusiones del Artículo}
    \framesubtitle{Implicaciones para el uso de LLMs en ABSA}
    \begin{itemize}
        \item Los LLMs pueden realizar ABSA, pero con variabilidad según el prompt.
        \item El mejor rendimiento se logra con \textbf{GPT-3.5 fine-tuned}.
        \item El fine-tuning reduce la sensibilidad al prompt y produce salidas más limpias.
        \item Los modelos pequeños siguen siendo superiores en coste.
        \item En general:  
        \textbf{Modelos grandes + fine-tuning → máximo rendimiento.}  
        \textbf{Modelos pequeños especializados → máxima eficiencia.}
    \end{itemize}
\end{frame}
	
	% SECCIÓN 3
	\section{Comparative Analysis of Deep Natural Networks and Large Language Models for Aspect-Based Sentiment Analysis}
	
    \begin{frame}
		\frametitle{Artículo 3: contexto y objetivo}
		\framesubtitle{Comparative Analysis of Deep Neural Networks and LLMs for ABSA}
		\begin{itemize}
			\item El artículo se centra en el \textbf{Aspect-Based Sentiment Analysis (ABSA)}, que asocia opiniones a aspectos concretos de un texto (p. ej. \textit{food quality}, \textit{staff} en una reseña).
			\item Distingue entre:
			\begin{itemize}
				\item \textbf{ATSA} (Aspect Term Sentiment Analysis): sentimiento sobre un término de aspecto explícito.
				\item \textbf{ACSA} (Aspect Category Sentiment Analysis): sentimiento sobre una categoría \textit{Entidad\#Atributo}, que puede no aparecer literalmente.
			\end{itemize}
			\item Problemas detectados en el estado del arte:
			\begin{itemize}
				\item Fuerte \textbf{dependencia del dominio}.
				\item Necesidad de gran cantidad de \textbf{datos etiquetados}.
			\end{itemize}
			\item Objetivo: comparar redes profundas “clásicas”, modelos ABSA basados en transformers y LLMs (PaLM, GPT-3.5) en distintas tareas de ABSA y dominios.
		\end{itemize}
	\end{frame}

    \begin{frame}
		\frametitle{Tareas de ABSA y preguntas de investigación}
		\framesubtitle{ATSA, ACSA y generalización}
		\begin{itemize}
			\item \textbf{Tareas principales estudiadas:}
			\begin{itemize}
				\item \textbf{ATSA}: predecir la polaridad del sentimiento hacia un término de aspecto dado.
				\item \textbf{ACSA}: predecir la polaridad de categorías como \textit{Food\#Quality}, \textit{Staff\#Service}, etc.
			\end{itemize}
			\item \textbf{Preguntas de investigación (simplificadas):}
			\begin{itemize}
				\item RQ1: ¿Cómo se generalizan los modelos ABSA clásicos (LSTM, BERT/DeBERTa) a \textbf{nuevos aspectos} o \textbf{nuevos dominios}?
				\item RQ2: ¿Hasta qué punto los \textbf{LLMs} (GPT-3.5, PaLM) se generalizan sin entrenamiento específico?
				\item RQ3: ¿Cómo se comparan en \textbf{rendimiento} y \textbf{coste} los LLMs frente a modelos entrenados específicamente para ABSA?
			\end{itemize}
		\end{itemize}
	\end{frame}

    \begin{frame}
		\frametitle{Metodología}
		\framesubtitle{Datasets y escenarios de evaluación}
		\begin{itemize}
			\item Se emplean varios conjuntos de datos anotados para ABSA, cubriendo distintos dominios:
			\begin{itemize}
				\item \textbf{SemEval 2016 Task 5}: restaurantes y portátiles.
				\item \textbf{MAMS}: oraciones con múltiples aspectos y sentimientos distintos.
				\item \textbf{Sentihood}: opiniones sobre barrios (aspectos como \textit{safety}, \textit{price}, \textit{nightlife}).
				\item \textbf{YASO} y \textbf{DOTSA}: colecciones multi-dominio (libros, ropa, hoteles, etc.).
			\end{itemize}
			\item \textbf{Escenarios de evaluación:}
			\begin{itemize}
				\item \textbf{In-domain}: entreno y pruebo en el mismo dominio.
				\item \textbf{Cross-domain}: entreno en un dominio y evalúo en otros para medir la \textbf{generalización}.
			\end{itemize}
			\item La comparación se basa en métricas estándar de clasificación: \textbf{accuracy} y \textbf{F1}.
		\end{itemize}
	\end{frame}
	
	\begin{frame}
		\frametitle{Modelos evaluados}
		\framesubtitle{LSTM, transformers para ABSA y LLMs}
		\begin{itemize}
			\item \textbf{Modelos LSTM “clásicos” de ABSA:}
			\begin{itemize}
				\item AE-LSTM, AT-LSTM y ATAE-LSTM (con atención y embeddings de aspecto).
			\end{itemize}
			\item \textbf{Modelos ABSA basados en transformers (fine-tuned):}
			\begin{itemize}
				\item \textbf{DeBERTa-v3-base-ABSA}: entrenado con un gran conjunto de ejemplos de ABSA.
				\item \textbf{flan-t5-large-ABSA}: modelo T5 ajustado para ABSA con datos generados y validados.
			\end{itemize}
			\item \textbf{LLMs genéricos sin entrenamiento específico:}
			\begin{itemize}
				\item \textbf{GPT-3.5-Turbo}.
				\item \textbf{PaLM text-bison-001}.
			\end{itemize}
			\item Los LLMs se evalúan mediante \textbf{prompts estructurados}: se da la reseña, lista de aspectos/categorías y se pide la polaridad de cada uno.
		\end{itemize}
	\end{frame}

    \begin{frame}
        \frametitle{Metodología}
        \framesubtitle{Evaluación de LLMs para subtareas de ABSA}
        \begin{figure}[htbp]
            \centering
            \includegraphics[width=0.9\textwidth]{fig_absa_methodology.png} % recorta la FIGURE 2 del paper
            \caption{Metodología de evaluación de ABSA para subtareas.}
        \end{figure}
    \end{frame}

    
    
    \begin{frame}
		\frametitle{Resultados: límites de los LSTM}
		\framesubtitle{Buen rendimiento local, mala transferencia}
		\begin{itemize}
			\item En \textbf{escenarios in-domain}, el mejor LSTM (ATAE-LSTM) supera a otros LSTM en accuracy y F1 para ATSA y ACSA.
			\item Sin embargo, en \textbf{cross-domain} la generalización es pobre:
			\begin{itemize}
				\item Modelos entrenados en ropa o libros fallan al pasar a hoteles o restaurantes.
				\item Solo hay buena transferencia entre dominios muy similares (p. ej. hotel \(\leftrightarrow\) restaurante).
			\end{itemize}
			\item Conclusión parcial:
			\begin{itemize}
				\item Los LSTM para ABSA siguen siendo \textbf{muy dependientes del dominio}.
				\item Requieren \textbf{reentrenar} o ajustar modelos distintos para cada nuevo dominio.
			\end{itemize}
		\end{itemize}
	\end{frame}

    \begin{frame}
		\frametitle{Resultados: transformers ABSA}
		\framesubtitle{DeBERTa-ABSA frente a LSTM}
		\begin{itemize}
			\item \textbf{DeBERTa-v3-ABSA} mejora claramente a los LSTM y a flan-t5-ABSA en la mayoría de datasets:
			\begin{itemize}
				\item Muy buen rendimiento en ATSA para hoteles y restaurantes.
				\item Resultados sólidos en datasets multi-dominio como YASO o Sentihood.
			\end{itemize}
			\item No obstante, se observan \textbf{limitaciones}:
			\begin{itemize}
				\item Peor rendimiento en dominios poco representados (p. ej. libros, ropa en DOTSA).
				\item Rendimiento más modesto en ACSA, donde hay que inferir categorías implícitas.
			\end{itemize}
			\item Mensaje: incluso los mejores modelos ABSA específicos siguen siendo \textbf{sensibles al dominio} y dependen de tener muchos ejemplos etiquetados.
		\end{itemize}
	\end{frame}

    \begin{frame}
		\frametitle{Resultados: LLMs (PaLM y GPT-3.5)}
		\framesubtitle{Rendimiento sin entrenamiento específico}
		\begin{itemize}
			\item \textbf{PaLM} obtiene resultados muy competitivos en muchas tareas de ATSA y ACSA:
			\begin{itemize}
				\item En varios dominios iguala o supera a DeBERTa-ABSA.
				\item Especialmente fuerte en hoteles, restaurantes y datasets multi-dominio.
			\end{itemize}
			\item \textbf{GPT-3.5} destaca en el dataset \textbf{MAMS}, más complejo (varios aspectos y polaridades por frase):
			\begin{itemize}
				\item Se aproxima al rendimiento de modelos especializados entrenados directamente en MAMS.
			\end{itemize}
			\item Implicación práctica:
			\begin{itemize}
				\item Los LLMs pueden ofrecer un \textbf{buen rendimiento zero-shot} en ABSA.
				\item Reducen el coste de etiquetar datos y entrenar un modelo distinto por dominio.
			\end{itemize}
		\end{itemize}
	\end{frame}

    \begin{frame}
	\frametitle{Conclusiones del Artículo 3}
	\framesubtitle{Comparative Analysis of DNNs and LLMs for ABSA}
	\begin{itemize}
		\item Para \textbf{tareas ABSA estándar} en dominios comunes, LLMs como PaLM o GPT-3.5 pueden alcanzar un rendimiento comparable al de los mejores modelos ABSA específicos, incluso sin entrenamiento adicional.
		\item Cuando hay \textbf{muchos datos etiquetados} y el dominio está bien definido, modelos especializados como DeBERTa-ABSA siguen ofreciendo el \textbf{mejor rendimiento global}.
		\item Los modelos basados en LSTM para ABSA muestran una \textbf{generalización cross-domain limitada}, funcionando bien solo en el dominio para el que han sido entrenados.
		\item En escenarios con \textbf{múltiples dominios} o pocos datos anotados, los LLMs son especialmente atractivos, al ofrecer un buen compromiso entre rendimiento y coste de desarrollo (ajustando el \textbf{prompt} en lugar de reentrenar modelos).
	\end{itemize}
\end{frame}

	% SECCIÓN 4
	\section{Conclusiones}

    \begin{frame}
      \frametitle{Conclusiones}
      \begin{itemize}
        \item \textbf{Madurez del SA con LLMs:} Los LLMs ya son muy competitivos en tareas sencillas de clasificación de sentimiento (SC), pero siguen por detrás de modelos especializados en tareas más complejas y estructuradas como ABSA avanzado o MAST.
        
        \item \textbf{Modelos especializados vs.\ LLMs finos:} DeBERTa-ABSA y otros modelos entrenados específicamente en datasets estándar logran un rendimiento muy alto en dominios conocidos, mientras que un GPT-3.5 finamente ajustado puede incluso marcar nuevo estado del arte en ABSA, a costa de muchos más parámetros y mayor coste computacional.
        
        \item \textbf{Ventaja clave de los LLMs:} Frente a redes profundas “clásicas”, LLMs como PaLM, GPT-3.5 o GPT-4 destacan en escenarios \emph{zero-/few-shot} y en entornos con muchos dominios o pocos datos etiquetados, ofreciendo buen rendimiento sin necesidad de reentrenar modelos para cada caso.

      \end{itemize}
    \end{frame}

    
    \begin{frame}
      \frametitle{Conclusiones}
      \begin{itemize}

        \item \textbf{Sensibilidad a \textit{prompts} y necesidad de nuevas métricas:} Los tres trabajos muestran que el rendimiento de los LLMs en SA/ABSA depende mucho del diseño del \textit{prompt} y de cómo se definen las reglas de anotación; además, benchmarks clásicos pueden infravalorar su capacidad, lo que motiva propuestas como SENTIEVAL y el análisis de errores más allá del F1.
        
        \item \textbf{Mensaje global:} En la práctica, ningún enfoque resulta claramente superior en todos los escenarios: los LLMs son una opción muy potente y flexible cuando hay pocos datos o muchos dominios, mientras que los modelos ABSA especializados (o LLMs finamente ajustados) siguen siendo preferibles cuando el dominio está bien definido, los requisitos de coste/latencia son estrictos y se necesita una salida muy estructurada y estable.
      \end{itemize}
    \end{frame}
    
	
	\begin{frame}
		\titlepage
	\end{frame}
	

	
\end{document}
